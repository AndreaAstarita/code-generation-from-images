{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer, one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences, TimeseriesGenerator\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Model, Sequential, model_from_json\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers.core import Dense, Dropout, Flatten\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.callbacks import ModelCheckpoint, CSVLogger\n",
    "from keras.layers import Embedding, TimeDistributed, RepeatVector, LSTM, concatenate , Input, Reshape, Dense, GRU, MaxPooling2D, BatchNormalization\n",
    "from keras.preprocessing.image import array_to_img, img_to_array, load_img\n",
    "from keras.utils import plot_model\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir_name = '/Users/KevinChuang/PycharmProjects/Screenshot-to-code-in-Keras/local/Bootstrap/train/'\n",
    "test_dir_name = '/Users/KevinChuang/PycharmProjects/Screenshot-to-code-in-Keras/local/Bootstrap/eval/'\n",
    "# Read a file and return a string\n",
    "def load_doc(filename):\n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "def load_data(data_dir):\n",
    "    text = []\n",
    "    images = []\n",
    "    # Load all the files and order them\n",
    "    all_filenames = listdir(data_dir)\n",
    "    all_filenames.sort()\n",
    "    for filename in (all_filenames):\n",
    "        if filename[-3:] == \"npz\":\n",
    "            # Load the images already prepared in arrays\n",
    "            image = np.load(data_dir+filename)\n",
    "            images.append(image['features'])\n",
    "        else:\n",
    "            # Load the boostrap tokens and rap them in a start and end tag\n",
    "            syntax = '<START> ' + load_doc(data_dir+filename) + ' <END>'\n",
    "            # Seperate all the words with a single space\n",
    "            syntax = ' '.join(syntax.split())\n",
    "            # Add a space after each comma\n",
    "            syntax = syntax.replace(',', ' ,')\n",
    "            text.append(syntax)\n",
    "    images = np.array(images, dtype=float)\n",
    "    return images, text\n",
    "\n",
    "train_features, train_texts = load_data(train_dir_name)\n",
    "test_features, test_texts = load_data(test_dir_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer word counts:  OrderedDict([(',', 1), ('{', 1), ('}', 1), ('small-title', 1), ('text', 1), ('quadruple', 1), ('row', 1), ('btn-inactive', 1), ('btn-orange', 1), ('btn-green', 1), ('btn-red', 1), ('double', 1), ('<START>', 1), ('header', 1), ('btn-active', 1), ('<END>', 1), ('single', 1)])\n",
      "Tokenizer document count:  1\n",
      "Tokenizer word index:  {',': 1, '{': 2, '}': 3, 'small-title': 4, 'text': 5, 'quadruple': 6, 'row': 7, 'btn-inactive': 8, 'btn-orange': 9, 'btn-green': 10, 'btn-red': 11, 'double': 12, '<START>': 13, 'header': 14, 'btn-active': 15, '<END>': 16, 'single': 17}\n",
      "Tokenizer word docs:  {'btn-orange': 1, '<END>': 1, 'header': 1, 'single': 1, 'btn-red': 1, '<START>': 1, 'quadruple': 1, '{': 1, 'text': 1, ',': 1, 'row': 1, 'small-title': 1, 'btn-active': 1, 'btn-inactive': 1, 'double': 1, '}': 1, 'btn-green': 1}\n"
     ]
    }
   ],
   "source": [
    "# Initialize the function to create the vocabulary \n",
    "tokenizer = Tokenizer(filters='', split=\" \", lower=False)\n",
    "# Create the vocabulary \n",
    "tokenizer.fit_on_texts([load_doc('/Users/KevinChuang/PycharmProjects/Screenshot-to-code-in-Keras/local/Bootstrap/resources/bootstrap.vocab')])\n",
    "\n",
    "# summarize what was learned\n",
    "print('Tokenizer word counts: ', tokenizer.word_counts)\n",
    "print('Tokenizer document count: ', tokenizer.document_count)\n",
    "print('Tokenizer word index: ', tokenizer.word_index)\n",
    "print('Tokenizer word docs: ', tokenizer.word_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<START> header { btn-inactive , btn-inactive , btn-inactive , btn-active } row { single { small-title , text , btn-red } } row { quadruple { small-title , text , btn-green } quadruple { small-title , text , btn-orange } quadruple { small-title , text , btn-red } quadruple { small-title , text , btn-orange } } row { double { small-title , text , btn-red } double { small-title , text , btn-orange } } <END>\n",
      "[[13, 14, 2, 8, 1, 8, 1, 8, 1, 15, 3, 7, 2, 17, 2, 4, 1, 5, 1, 11, 3, 3, 7, 2, 6, 2, 4, 1, 5, 1, 10, 3, 6, 2, 4, 1, 5, 1, 9, 3, 6, 2, 4, 1, 5, 1, 11, 3, 6, 2, 4, 1, 5, 1, 9, 3, 3, 7, 2, 12, 2, 4, 1, 5, 1, 11, 3, 12, 2, 4, 1, 5, 1, 9, 3, 3, 16]]\n"
     ]
    }
   ],
   "source": [
    "# # Add one spot for the empty word in the vocabulary \n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "max_length = 48\n",
    "print(train_texts[0])\n",
    "print(tokenizer.texts_to_sequences([train_texts[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(texts, features, max_sequence):\n",
    "    X, y, image_data = list(), list(), list()\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    for img_no, seq in enumerate(sequences):\n",
    "        for i in range(1, len(seq)):\n",
    "            # Add the sentence until the current count(i) and add the current count to the output\n",
    "            in_seq, out_seq = seq[:i], seq[i]\n",
    "            # Pad all the input token sentences to max_sequence\n",
    "            in_seq = pad_sequences([in_seq], maxlen=max_sequence)[0]\n",
    "            # Turn the output into one-hot encoding\n",
    "            out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "            # Add the corresponding image to the boostrap token file\n",
    "            image_data.append(features[img_no])\n",
    "            # Cap the input sentence to 48 tokens and add it\n",
    "            X.append(in_seq[-48:])\n",
    "            y.append(out_seq)\n",
    "    return np.array(image_data), np.array(X), np.array(y)\n",
    "\n",
    "# image_data, x_sequence, labels = preprocess_data([train_texts[0]], [train_features[0]], 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data generator, intended to be used in a call to model.fit_generator()\n",
    "# This will save memory and prevent MemoryErrors from numpy\n",
    "def data_generator(descriptions, features, n_step, max_sequence):\n",
    "    '''n_steps = batch_size'''\n",
    "    # loop until we finish training\n",
    "#     print('Data Generator initialized...')\n",
    "    while 1:\n",
    "        # loop over photo identifiers in the dataset\n",
    "        for i in range(0, len(descriptions), n_step):\n",
    "            batch = 0\n",
    "            Ximages, XSeq, y = list(), list(),list()\n",
    "            for j in range(i, min(len(descriptions), i+n_step)):\n",
    "                image = features[j]\n",
    "                # retrieve text input\n",
    "                desc = descriptions[j]\n",
    "                # generate input-output pairs\n",
    "                in_img, in_seq, out_word = preprocess_data([desc], [image], max_sequence)\n",
    "                for k in range(len(in_img)):\n",
    "                    Ximages.append(in_img[k])\n",
    "                    XSeq.append(in_seq[k])\n",
    "                    y.append(out_word[k])\n",
    "            # yield this batch of samples to the model\n",
    "#             print('Yielding batch %d \\n' % i)\n",
    "            yield [[np.array(Ximages), np.array(XSeq)], np.array(y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(76, 256, 256, 3)\n",
      "(76, 48)\n",
      "(76, 18)\n"
     ]
    }
   ],
   "source": [
    "generator = data_generator(train_texts, train_features, 1, 150)\n",
    "x, y =next(generator)\n",
    "print(x[0].shape)\n",
    "print(x[1].shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Using real-time data augmentation.')\n",
    "# This will do preprocessing and realtime data augmentation:\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "    samplewise_center=False,  # set each sample mean to 0\n",
    "    featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "    samplewise_std_normalization=False,  # divide each input by its std\n",
    "    zca_whitening=False,  # apply ZCA whitening\n",
    "    zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
    "    rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    # randomly shift images horizontally (fraction of total width)\n",
    "    width_shift_range=0.1,\n",
    "    # randomly shift images vertically (fraction of total height)\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.,  # set range for random shear\n",
    "    zoom_range=0.,  # set range for random zoom\n",
    "    channel_shift_range=0.,  # set range for random channel shifts\n",
    "    # set mode for filling points outside the input boundaries\n",
    "    fill_mode='nearest',\n",
    "    cval=0.,  # value used for fill_mode = \"constant\"\n",
    "    horizontal_flip=True,  # randomly flip images\n",
    "    vertical_flip=False,  # randomly flip images\n",
    "    # set rescaling factor (applied before any other transformation)\n",
    "    rescale=None,\n",
    "    # set function that will be applied on each input\n",
    "    preprocessing_function=None,\n",
    "    # image data format, either \"channels_first\" or \"channels_last\"\n",
    "    data_format=None,\n",
    "    # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "    validation_split=0.0)\n",
    "\n",
    "# Compute quantities required for feature-wise normalization\n",
    "# (std, mean, and principal components if ZCA whitening is applied).\n",
    "datagen.fit(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "params = {'dim': (256, 256),\n",
    "          'batch_size': 1,\n",
    "          'n_classes': 18,\n",
    "          'n_channels': 1,\n",
    "          'shuffle': True}\n",
    "\n",
    "partition = {'train': [i for i in os.listdir(train_dir_name) if i[-3:] == '.npz'],\n",
    "             'validation': [v for v in os.listdir(test_dir_name) if v[-3:] == '.npz']}\n",
    "labels = {i: i % 10 for i in range(n_samples+100)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "\n",
    "    # initialization\n",
    "    def __init__(self, dim_x = 256, dim_y = 256, batch_size = 32, number_features = 48, shuffle = True):\n",
    "\n",
    "        self.dim_x = dim_x\n",
    "        self.dim_y = dim_y\n",
    "        self.batch_size = batch_size\n",
    "        self.number_features = number_features\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "    def __len__(self):\n",
    "      'Denotes the number of batches per epoch'\n",
    "      return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "        \n",
    "    # randomize the indices of the list_IDs\n",
    "    def __get_exploration_order(self, list_IDs):\n",
    "\n",
    "        indices = np.arange(len(list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(indices)\n",
    "        return indices\n",
    "\n",
    "    # import the batch based on IDs\n",
    "    def __data_generation(self, labels, list_IDs_temp):\n",
    "        # initialization\n",
    "        X1 = np.empty((self.batch_size, self.dim_x, self.dim_y, 3))\n",
    "        X2 = np.empty((self.batch_size, self.number_features))\n",
    "        Y = np.empty((self.batch_size), dtype = int)\n",
    "\n",
    "        # generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            # import picture, resize and convert to array\n",
    "            img = Image.open(paths[ID])\n",
    "            arr = np.array(img)\n",
    "            img_resized = imresize(arr, (self.dim_x, self.dim_y))\n",
    "\n",
    "            # store picture\n",
    "            X1[i, :, :, :] = img_resized / 255\n",
    "\n",
    "            # get 'non-picture' features\n",
    "            X2[i] = features[ID]\n",
    "\n",
    "            # store class\n",
    "            Y[i] = labels[ID]\n",
    "\n",
    "        return X1, X2, to_categorical([Y], num_classes=18)[0]\n",
    "\n",
    "    # an infinite loop that goes through the dataset and outputs one batch at a time\n",
    "    def generate(self, labels, list_IDs):\n",
    "\n",
    "        # infinite loop\n",
    "        while 1:\n",
    "\n",
    "            # generate order of exploration of dataset\n",
    "            indices = self.__get_exploration_order(list_IDs)\n",
    "\n",
    "            # generate batches\n",
    "            imax = int(len(indices) / self.batch_size)\n",
    "\n",
    "            for i in range(imax):\n",
    "            # find list of IDs\n",
    "            list_IDs_temp = [list_IDs[k] for k in indices[i * self.batch_size: (i + 1) * self.batch_size]]\n",
    "\n",
    "            # generate data\n",
    "            X1, X2, Y = self.__data_generation(labels, list_IDs_temp)\n",
    "\n",
    "            yield [X1, X2], Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For convolution layers, remember that the output shape is: [(W-F)/S +1, (W-F)/S +1], \n",
    "# # where W=H= 256 and filter shape F=3, and stride S=1 or 2\n",
    "\n",
    "# # Create the encoder\n",
    "image_model = Sequential()\n",
    "image_model.add(Conv2D(16, (3,3), padding='valid', activation='relu', input_shape=(256, 256, 3,)))\n",
    "image_model.add(Conv2D(16, (3,3), activation='relu', padding='same'))\n",
    "image_model.add(MaxPooling2D(pool_size=(2, 2), strides=2))\n",
    "image_model.add(Conv2D(32, (3,3), activation='relu', padding='same'))\n",
    "image_model.add(Conv2D(32, (3,3), activation='relu', padding='same'))\n",
    "image_model.add(MaxPooling2D(pool_size=(2, 2), strides=2))\n",
    "image_model.add(Conv2D(64, (3,3), activation='relu', padding='same'))\n",
    "image_model.add(Conv2D(64, (3,3), activation='relu', padding='same'))\n",
    "image_model.add(MaxPooling2D(pool_size=(2, 2), strides=2))\n",
    "image_model.add(Conv2D(128, (3,3), activation='relu', padding='same'))\n",
    "\n",
    "image_model.add(Flatten())\n",
    "image_model.add(Dense(1024, activation='relu'))\n",
    "image_model.add(Dropout(0.3))\n",
    "image_model.add(Dense(1024, activation='relu'))\n",
    "image_model.add(Dropout(0.3))\n",
    "\n",
    "image_model.add(RepeatVector(max_length))\n",
    "\n",
    "visual_input = Input(shape=(256, 256, 3,))\n",
    "encoded_image = image_model(visual_input)\n",
    "\n",
    "language_input = Input(shape=(max_length,))\n",
    "language_model = Embedding(vocab_size, 50, input_length=max_length, mask_zero=True)(language_input)\n",
    "language_model = GRU(128, return_sequences=True)(language_model)\n",
    "language_model = GRU(128, return_sequences=True)(language_model)\n",
    "\n",
    "#Create the decoder\n",
    "decoder = concatenate([encoded_image, language_model])\n",
    "decoder = GRU(512, return_sequences=True)(decoder)\n",
    "decoder = GRU(512, return_sequences=False)(decoder)\n",
    "decoder = Dense(vocab_size, activation='softmax')(decoder)\n",
    "\n",
    "# Compile the model\n",
    "model = Model(inputs=[visual_input, language_input], outputs=decoder)\n",
    "# optimizer = RMSprop(lr=0.0001, clipvalue=1.0)\n",
    "optimizer = Adam(lr=0.0001, clipvalue=1.0)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# plot_model(model, to_file='model.png')\n",
    "\n",
    "# Serialize Model to JSON\n",
    "# model_json = model.to_json()\n",
    "# with open('GRU_model.json', 'w') as json_file:\n",
    "#     json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For <keras.layers.convolutional.Conv2D object at 0x1113d07f0>,\n",
      "\tinput_shape:  (None, 256, 256, 3)\n",
      "\toutput_shape:  (None, 252, 252, 32)\n",
      "For <keras.layers.convolutional.Conv2D object at 0x1113d0908>,\n",
      "\tinput_shape:  (None, 252, 252, 32)\n",
      "\toutput_shape:  (None, 252, 252, 32)\n",
      "For <keras.layers.pooling.MaxPooling2D object at 0x10775e7f0>,\n",
      "\tinput_shape:  (None, 252, 252, 32)\n",
      "\toutput_shape:  (None, 126, 126, 32)\n",
      "For <keras.layers.core.Dropout object at 0x1113d0e10>,\n",
      "\tinput_shape:  (None, 126, 126, 32)\n",
      "\toutput_shape:  (None, 126, 126, 32)\n",
      "For <keras.layers.convolutional.Conv2D object at 0x1077799b0>,\n",
      "\tinput_shape:  (None, 126, 126, 32)\n",
      "\toutput_shape:  (None, 126, 126, 64)\n",
      "For <keras.layers.convolutional.Conv2D object at 0x1113d0b70>,\n",
      "\tinput_shape:  (None, 126, 126, 64)\n",
      "\toutput_shape:  (None, 126, 126, 64)\n",
      "For <keras.layers.pooling.MaxPooling2D object at 0x107dd0f28>,\n",
      "\tinput_shape:  (None, 126, 126, 64)\n",
      "\toutput_shape:  (None, 63, 63, 64)\n",
      "For <keras.layers.core.Dropout object at 0x10775ee80>,\n",
      "\tinput_shape:  (None, 63, 63, 64)\n",
      "\toutput_shape:  (None, 63, 63, 64)\n",
      "For <keras.layers.convolutional.Conv2D object at 0x107de0f28>,\n",
      "\tinput_shape:  (None, 63, 63, 64)\n",
      "\toutput_shape:  (None, 63, 63, 128)\n",
      "For <keras.layers.convolutional.Conv2D object at 0x107d9b550>,\n",
      "\tinput_shape:  (None, 63, 63, 128)\n",
      "\toutput_shape:  (None, 63, 63, 128)\n",
      "For <keras.layers.pooling.MaxPooling2D object at 0x123671518>,\n",
      "\tinput_shape:  (None, 63, 63, 128)\n",
      "\toutput_shape:  (None, 31, 31, 128)\n",
      "For <keras.layers.core.Dropout object at 0x107df2908>,\n",
      "\tinput_shape:  (None, 31, 31, 128)\n",
      "\toutput_shape:  (None, 31, 31, 128)\n",
      "For <keras.layers.core.Flatten object at 0x11137a358>,\n",
      "\tinput_shape:  (None, 31, 31, 128)\n",
      "\toutput_shape:  (None, 123008)\n",
      "For <keras.layers.core.Dense object at 0x107e08eb8>,\n",
      "\tinput_shape:  (None, 123008)\n",
      "\toutput_shape:  (None, 1024)\n",
      "For <keras.layers.core.Dropout object at 0x12363af98>,\n",
      "\tinput_shape:  (None, 1024)\n",
      "\toutput_shape:  (None, 1024)\n",
      "For <keras.layers.core.Dense object at 0x111497a90>,\n",
      "\tinput_shape:  (None, 1024)\n",
      "\toutput_shape:  (None, 1024)\n",
      "For <keras.layers.core.Dropout object at 0x111497c50>,\n",
      "\tinput_shape:  (None, 1024)\n",
      "\toutput_shape:  (None, 1024)\n",
      "For <keras.layers.core.RepeatVector object at 0x1114aa400>,\n",
      "\tinput_shape:  (None, 1024)\n",
      "\toutput_shape:  (None, 48, 1024)\n"
     ]
    }
   ],
   "source": [
    "for l in image_model.layers:\n",
    "    print('For %s,' % l)\n",
    "    print('\\tinput_shape: ', l.input_shape)\n",
    "    print('\\toutput_shape: ', l.output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For <keras.engine.input_layer.InputLayer object at 0x111506160>,\n",
      "\tinput_shape:  (None, 48)\n",
      "\toutput_shape:  (None, 48)\n",
      "For <keras.layers.embeddings.Embedding object at 0x111506588>,\n",
      "\tinput_shape:  (None, 48)\n",
      "\toutput_shape:  (None, 48, 50)\n",
      "For <keras.engine.input_layer.InputLayer object at 0x1114aada0>,\n",
      "\tinput_shape:  (None, 256, 256, 3)\n",
      "\toutput_shape:  (None, 256, 256, 3)\n",
      "For <keras.layers.recurrent.GRU object at 0x1114f4780>,\n",
      "\tinput_shape:  (None, 48, 50)\n",
      "\toutput_shape:  (None, 48, 128)\n",
      "For <keras.engine.sequential.Sequential object at 0x1113d0828>,\n",
      "\tinput_shape:  (None, 256, 256, 3)\n",
      "\toutput_shape:  (None, 48, 1024)\n",
      "For <keras.layers.recurrent.GRU object at 0x111522ba8>,\n",
      "\tinput_shape:  (None, 48, 128)\n",
      "\toutput_shape:  (None, 48, 128)\n",
      "For <keras.layers.merge.Concatenate object at 0x111678f60>,\n",
      "\tinput_shape:  [(None, 48, 1024), (None, 48, 128)]\n",
      "\toutput_shape:  (None, 48, 1152)\n",
      "For <keras.layers.recurrent.GRU object at 0x111a4eda0>,\n",
      "\tinput_shape:  (None, 48, 1152)\n",
      "\toutput_shape:  (None, 48, 512)\n",
      "For <keras.layers.recurrent.GRU object at 0x111a88cc0>,\n",
      "\tinput_shape:  (None, 48, 512)\n",
      "\toutput_shape:  (None, 512)\n",
      "For <keras.layers.core.Dense object at 0x111df9be0>,\n",
      "\tinput_shape:  (None, 512)\n",
      "\toutput_shape:  (None, 18)\n"
     ]
    }
   ],
   "source": [
    "for l in model.layers:\n",
    "    print('For %s,' % l)\n",
    "    print('\\tinput_shape: ', l.input_shape)\n",
    "    print('\\toutput_shape: ', l.output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filepath= \"weights/weights.epoch-{epoch:02d}--loss-{loss:.4f}--acc-{acc:.4f}--val_loss-{val_loss:.4f}--val_acc-{val_acc:.4f}.hdf5\"\n",
    "filepath=\"weights/weights.epoch-{epoch:02d}--loss-{loss:.4f}--acc-{acc:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath,verbose=1, \n",
    "                             save_weights_only=True, period=2)\n",
    "\n",
    "# Create a callback that streams epoch results to a csv file.\n",
    "csv_file = 'weights/training.log'\n",
    "csv_logger = CSVLogger(csv_file)\n",
    "\n",
    "\n",
    "callbacks_list = [checkpoint, csv_logger]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define the neural network hyperparameters here as a dict\n",
    "# steps_per_epoch = # of training examples / batch_size\n",
    "# steps_per_epoch = number of batches and number of times the weights are updated (backpropagation occurs to update weights)\n",
    "config = {}\n",
    "config['steps_per_epoch'] = 1500\n",
    "config['epochs'] = 50\n",
    "config['batch_size'] = 1500/config['steps_per_epoch']\n",
    "config['validation_steps'] = 250\n",
    "config_path = 'weights/config.json'\n",
    "\n",
    "with open(config_path, 'w') as fh:\n",
    "    json.dump(config, fh, indent=4, sort_keys=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for e in range(config['epochs']):\n",
    "# #     print(\"epoch %d\" % e)\n",
    "#     for X_train, Y_train in data_generator(train_texts, train_features, 2, 150): # these comes in batches of 2 images\n",
    "#         model.fit(X_train, Y_train, steps_per_epoch=750)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "   1/1500 [..............................] - ETA: 16:15:30 - loss: 2.8978 - acc: 0.0395"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-23554adb135f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m                                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                                     workers=5)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1424\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1425\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1426\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1428\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    189\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    190\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1218\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1220\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1221\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2659\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2661\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2662\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2663\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2629\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2630\u001b[0m                                 session)\n\u001b[0;32m-> 2631\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2632\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1449\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[0;32m-> 1451\u001b[0;31m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "model_history = model.fit_generator(data_generator(train_texts, train_features, 1, 150),                             \n",
    "                                    steps_per_epoch=config['steps_per_epoch'], \n",
    "                                    epochs=config['epochs'], \n",
    "                                    callbacks=callbacks_list, \n",
    "                                    verbose=1,\n",
    "                                    workers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train and fit the model using a generator (more memory efficient)\n",
    "# # steps_per_epoch = # of training examples / batch_size\n",
    "# n_steps = batch\n",
    "# model_history = model.fit_generator(data_generator(train_texts, train_features, 1, 150),                             \n",
    "#                                     steps_per_epoch=config['steps_per_epoch'], \n",
    "#                                     epochs=config['epochs'], \n",
    "#                                     callbacks=callbacks_list, \n",
    "#                                     verbose=1,\n",
    "#                                     workers=5,\n",
    "#                                     validation_data=data_generator(test_texts, test_features, 1, 150),\n",
    "#                                    validation_steps=config['validation_steps'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_history = model.fit_generator(data_generator(train_texts, train_features, 1, 150),                             \n",
    "#                                     steps_per_epoch=config['steps_per_epoch'], \n",
    "#                                     epochs=config['epochs'], \n",
    "#                                     callbacks=callbacks_list, \n",
    "#                                     verbose=1,\n",
    "#                                     use_multiprocessing=True,\n",
    "#                                     validation_data=data_generator(test_texts, test_features, 1, 150),\n",
    "#                                    validation_steps=config['validation_steps'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file\n",
    "with open('K:\\\\Projects\\\\Autogeneration_of_Code_from_Images\\\\weights\\\\report.txt','w') as fh:\n",
    "    # Pass the file handle in as a lambda function to make it callable\n",
    "    model_history.model.summary(print_fn=lambda x: fh.write(x + '\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
